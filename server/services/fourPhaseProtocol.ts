// EXACT USER-SPECIFIED 4-PHASE INTELLIGENCE EVALUATION PROTOCOL

const EXACT_COMPLETE_QUESTIONS = `IS IT INSIGHTFUL? 
DOES IT DEVELOP POINTS? (OR, IF IT IS A SHORT EXCERPT, IS THERE EVIDENCE THAT IT WOULD DEVELOP POINTS IF EXTENDED)? 
IS THE ORGANIZATION MERELY SEQUENTIAL (JUST ONE POINT AFTER ANOTHER, LITTLE OR NO LOGICAL SCAFFOLDING)? OR ARE THE IDEAS ARRANGED, NOT JUST SEQUENTIALLY BUT HIERARCHICALLY? 
IF THE POINTS IT MAKES ARE NOT INSIGHTFUL, DOES IT OPERATE SKILLFULLY WITH CANONS OF LOGIC/REASONING. 
ARE THE POINTS CLICHES? OR ARE THEY "FRESH"? 
DOES IT USE TECHNICAL JARGON TO OBFUSCATE OR TO RENDER MORE PRECISE? 
IS IT ORGANIC? DO POINTS DEVELOP IN AN ORGANIC, NATURAL WAY? DO THEY 'UNFOLD'? OR ARE THEY FORCED AND ARTIFICIAL? 
DOES IT OPEN UP NEW DOMAINS? OR, ON THE CONTRARY, DOES IT SHUT OFF INQUIRY (BY CONDITIONALIZING FURTHER DISCUSSION OF THE MATTERS ON ACCEPTANCE OF ITS INTERNAL AND POSSIBLY VERY FAULTY LOGIC)? 
IS IT ACTUALLY INTELLIGENT OR JUST THE WORK OF SOMEBODY WHO, JUDGING BY THE SUBJECT-MATTER, IS PRESUMED TO BE INTELLIGENT (BUT MAY NOT BE)? 
IS IT REAL OR IS IT PHONY? 
DO THE SENTENCES EXHIBIT COMPLEX AND COHERENT INTERNAL LOGIC? 
IS THE PASSAGE GOVERNED BY A STRONG CONCEPT? OR IS THE ONLY ORGANIZATION DRIVEN PURELY BY EXPOSITORY (AS OPPOSED TO EPISTEMIC) NORMS?
IS THERE SYSTEM-LEVEL CONTROL OVER IDEAS? IN OTHER WORDS, DOES THE AUTHOR SEEM TO RECALL WHAT HE SAID EARLIER AND TO BE IN A POSITION TO INTEGRATE IT INTO POINTS HE HAS MADE SINCE THEN? 
ARE THE POINTS 'REAL'? ARE THEY FRESH? OR IS SOME INSTITUTION OR SOME ACCEPTED VEIN OF PROPAGANDA OR ORTHODOXY JUST USING THE AUTHOR AS A MOUTH PIECE?
IS THE WRITING EVASIVE OR DIRECT? 
ARE THE STATEMENTS AMBIGUOUS? 
DOES THE PROGRESSION OF THE TEXT DEVELOP ACCORDING TO WHO SAID WHAT OR ACCORDING TO WHAT ENTAILS OR CONFIRMS WHAT? 
DOES THE AUTHOR USER OTHER AUTHORS TO DEVELOP HIS IDEAS OR TO CLOAK HIS OWN LACK OF IDEAS?

ADDITIONAL CRITICAL QUESTIONS:
ARE THERE TERMS THAT ARE UNDEFINED BUT SHOULD BE DEFINED, IN THE SENSE THAT, WITHOUT DEFINITIONS, IT IS DIFFICULT OR IMPOSSIBLE TO KNOW WHAT IS BEING SAID OR THEREFORE TO EVALUATE WHAT IS BEING SAID?
ARE THERE "FREE VARIABLES" IN THE TEXT? IE ARE THERE QUALIFICATIONS OR POINTS THAT ARE MADE BUT DO NOT CONNECT TO ANYTHING LATER OR EARLIER?
DO NEW STATEMENTS DEVELOP OUT OF OLD ONES? OR ARE THEY MERELY "ADDED" TO PREVIOUS ONES, WITHOUT IN ANY SENSE BEING GENERATED BY THEM?
DO NEW STATEMENTS CLARIFY OR DO THEY LEAD TO MORE LACK OF CLARITY?
IS THE PASSAGE ACTUALLY (PALPABLY) SMART? OR IS ONLY "PRESUMPTION-SMART"?
IF YOUR JUDGMENT IS THAT IT IS INSIGHTFUL, CAN YOU STATEMENT THAT INSIGHT IN A SINGLE SENTENCE?
HOW WELL DOES IT MAKE ITS CASE?
IF I WERE TO GIVE A HIGH SCORE TO THIS PASSAGE, WOULD I BE REWARDING IMPOSTOR SCAFFOLDING?
IF I WERE TO GIVE A HIGH SCORE TO THIS PASSAGE, WOULD I BE REWARDING CONFORMITY TO ACADEMIC/BUREAUCRATIC NORMS?
IF I WERE TO GIVE A LOW SCORE TO THIS PASSAGE, WOULD I BE PENALIZING ACTUAL INTELLIGENCE OWING TO A LACK OF CONFORMITY TO ACADEMIC/BUREAUCRATIC NORMS?`;

// PHASE 1: COMPREHENSIVE - Ultra-detailed evidence-based analysis
function createPhase1ComprehensivePrompt(text: string, questions: string): string {
  return `COMPREHENSIVE INTELLIGENCE ANALYSIS - DETAILED EVIDENCE-BASED EVALUATION

CRITICAL INSTRUCTIONS:
- This is a COMPREHENSIVE analysis requiring EXTENSIVE detail and evidence
- EVERY single statement you make MUST be supported by DIRECT QUOTES from the text
- Your analysis must be AT LEAST 3000 words
- For EACH of the ${questions.split('\n').filter(q => q.trim().length > 0).length} questions below, you MUST:
  1. Quote MULTIPLE relevant passages (minimum 2-3 quotes per question)
  2. Provide DETAILED arguments (minimum 100 words per question)
  3. Show EXACTLY how the evidence supports your judgment

SCORING PRINCIPLES:
- Score N/100 means author is smarter than N% of general population
- 83/100 = only 17/100 people in Walmart are smarter
- Judge RAW INTELLIGENCE, not academic polish
- Brilliant but unconventional = HIGH score
- Polished but dumb = LOW score

COMPREHENSIVE ANALYSIS FORMAT:

For EACH question below, you must write:

QUESTION: [restate the question]

EVIDENCE FROM TEXT:
Quote 1: "[exact quote from text]"
Quote 2: "[exact quote from text]"  
Quote 3: "[exact quote from text]"

DETAILED ANALYSIS:
[Write 100-200 words analyzing how these quotes answer the question. Be specific about what the quotes reveal about intelligence. Reference the exact words and phrases. Explain the logical connections. Discuss what this says about the author's cognitive abilities.]

JUDGMENT: [Your assessment for this specific question]

---

${questions}

REQUIREMENTS:
- Answer ALL questions with the format above
- Minimum 3 quotes per question
- Minimum 100 words analysis per question
- Total response must exceed 3000 words
- Be brutally honest about intelligence level
- Support EVERY claim with textual evidence

After analyzing all questions, provide:

OVERALL INTELLIGENCE ASSESSMENT: [500+ word summary integrating all evidence]

FINAL SCORE: [number]/100

TEXT TO ANALYZE:
${text}`;
}

// PHASE 1: QUICK - Brief analysis for fast mode
function createPhase1Prompt(text: string, questions: string): string {
  return `QUICK INTELLIGENCE ANALYSIS

Score N/100 means author is smarter than N% of general population.
Judge RAW INTELLIGENCE only, not academic polish.

${questions}

Provide brief assessment with key evidence. Include quotes to support major points.

FINAL SCORE: [number]/100

TEXT:
${text}`;
}

// PHASE 2: COMPREHENSIVE pushback with detailed re-analysis
function createPhase2ComprehensivePrompt(score: number, text: string, questions: string): string {
  const peopleOutperforming = 100 - score;
  return `PHASE 2: COMPREHENSIVE PUSHBACK AND RE-EVALUATION

Your initial position was that ${peopleOutperforming}/100 people outperform this author in intelligence.
That means ${peopleOutperforming} out of 100 people in Walmart are running rings around this person.

Are you SURE about that assessment? Re-analyze the text with even MORE rigorous evidence.

${questions}

COMPREHENSIVE RE-ANALYSIS REQUIREMENTS:
- For EACH question, provide 2-3 NEW quotes you may have overlooked
- Write 150+ words per question analyzing the evidence
- Be MORE critical and look for intelligence signals you may have missed
- Challenge your initial assessment with fresh evidence
- Total response must exceed 2500 words

Use the same detailed format as Phase 1.

FINAL SCORE: [NUMBER]/100

TEXT:
${text}`;
}

// PHASE 2: Quick pushback for fast mode  
function createPhase2Prompt(score: number, text: string, questions: string): string {
  const peopleOutperforming = 100 - score;
  return `Your position is that ${peopleOutperforming}/100 outperform the author. Are you sure?

${questions}

Brief re-assessment with key evidence.

FINAL SCORE: [NUMBER]/100

TEXT:
${text}`;
}

// PHASE 3: Walmart metric check exactly as specified
function createPhase3Prompt(score: number): string {
  const peopleOutperforming = 100 - score;
  return `You ask the LLM if its numerical scores (N/100, e.g. 99/100, 42/100) are consistent with the fact that those are to be taken to mean that (100-N) people out of 100 outperform the author in the relevant respect. So if a score of ${score}/100 is awarded to a paper, that means that ${peopleOutperforming}/100 people in Walmart are running rings around this person. Is this consistent with your assessment?

FINAL SCORE: [NUMBER]/100`;
}

// PHASE 4: Accept and report what the LLM says
function createPhase4Prompt(): string {
  return `At this point, you accept and report what the LLM says.

FINAL SCORE: [NUMBER]/100`;
}

// Text chunking function for large texts - 500 words per chunk
function chunkText(text: string, maxWordsPerChunk: number = 500): string[] {
  const words = text.split(' ');
  const chunks: string[] = [];
  
  for (let i = 0; i < words.length; i += maxWordsPerChunk) {
    const chunk = words.slice(i, i + maxWordsPerChunk).join(' ');
    if (chunk.trim()) {
      chunks.push(chunk.trim());
    }
  }
  
  return chunks;
}

// Generic LLM caller
async function callLLMProvider(
  provider: 'openai' | 'anthropic' | 'perplexity' | 'deepseek',
  messages: Array<{role: string, content: string}>
): Promise<string> {
  try {
    if (provider === 'openai') {
      const OpenAI = (await import('openai')).default;
      const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
      
      const completion = await openai.chat.completions.create({
        model: "gpt-4o",
        messages: messages as any,
        temperature: 0.1
      });
      
      return completion.choices[0]?.message?.content || '';
    } else if (provider === 'anthropic') {
      const Anthropic = (await import('@anthropic-ai/sdk')).default;
      const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY });
      
      const completion = await anthropic.messages.create({
        model: "claude-3-5-sonnet-20241022",
        max_tokens: 4000,
        messages: messages as any,
        temperature: 0.1
      });
      
      return completion.content[0]?.type === 'text' ? completion.content[0].text : '';
    } else if (provider === 'perplexity') {
      const response = await fetch('https://api.perplexity.ai/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${process.env.PERPLEXITY_API_KEY}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: "sonar",
          messages: messages,
          temperature: 0.1
        })
      });
      
      const data = await response.json();
      return data.choices?.[0]?.message?.content || '';
    } else if (provider === 'deepseek') {
      const response = await fetch('https://api.deepseek.com/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${process.env.DEEPSEEK_API_KEY}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: "deepseek-chat",
          messages: messages,
          temperature: 0.1
        })
      });
      
      const data = await response.json();
      return data.choices?.[0]?.message?.content || '';
    }
    
    return '';
  } catch (error) {
    console.error(`Error calling ${provider}:`, error);
    return '';
  }
}

// Score extraction function
function extractScore(text: string): number {
  console.log(`EXTRACTING SCORE FROM RESPONSE LENGTH: ${text.length}`);
  console.log(`EXTRACTING SCORE - LAST 500 CHARS: "${text.slice(-500)}"`);
  
  // Try multiple patterns in order of specificity
  
  // 1. Look for "FINAL SCORE: X/100" (case insensitive)
  const finalScoreMatch = text.match(/FINAL\s*SCORE:\s*(\d+)\s*\/\s*100/i);
  if (finalScoreMatch) {
    const score = parseInt(finalScoreMatch[1]);
    console.log(`✓ EXTRACTED FINAL SCORE FORMAT: ${score}/100`);
    return score;
  }
  
  // 2. Look for "Score: X/100" or "score X/100"
  const scoreMatch = text.match(/\bscore[:\s]+(\d+)\s*\/\s*100/i);
  if (scoreMatch) {
    const score = parseInt(scoreMatch[1]);
    console.log(`✓ EXTRACTED SCORE FORMAT: ${score}/100`);
    return score;
  }
  
  // 3. Look for any "X/100" pattern, prefer the last one
  const allScores = text.match(/\b(\d+)\s*\/\s*100\b/g);
  if (allScores && allScores.length > 0) {
    const lastMatch = allScores[allScores.length - 1];
    const score = parseInt(lastMatch.split('/')[0].trim());
    console.log(`✓ EXTRACTED NUMERICAL SCORE (last occurrence): ${score}/100 from ${allScores.length} matches`);
    return score;
  }
  
  // 4. Look for standalone number near end if it's reasonable (1-100)
  const endNumbers = text.slice(-1000).match(/\b(\d+)\b/g);
  if (endNumbers) {
    for (let i = endNumbers.length - 1; i >= 0; i--) {
      const num = parseInt(endNumbers[i]);
      if (num >= 1 && num <= 100) {
        console.log(`⚠ EXTRACTED STANDALONE NUMBER: ${num} (fallback)`);
        return num;
      }
    }
  }
  
  console.log('❌ NO SCORE FOUND, defaulting to 0');
  return 0;
}

// NORMAL PROTOCOL - Phase 1 only
export async function executeNormalProtocol(
  text: string,
  provider: 'openai' | 'anthropic' | 'perplexity' | 'deepseek'
): Promise<any> {
  console.log(`NORMAL INTELLIGENCE ANALYSIS WITH ${provider.toUpperCase()} - PHASE 1 ONLY`);
  console.log(`EXECUTING PHASE 1 ONLY FOR INTELLIGENCE WITH ${provider.toUpperCase()}`);
  
  const questions = EXACT_COMPLETE_QUESTIONS;
  
  // PHASE 1: Initial evaluation
  const phase1Prompt = createPhase1Prompt(text, questions);
  const phase1Response = await callLLMProvider(provider, [
    { role: 'user', content: phase1Prompt }
  ]);
  let finalScore = extractScore(phase1Response);
  
  console.log(`PHASE 1 COMPLETE: Score ${finalScore}/100`);
  
  const cleanedResponse = phase1Response.replace(/\*{1,3}/g, '').replace(/#{1,6}\s*/g, '').trim();
  console.log(`PHASE 1 RESPONSE PREVIEW: "${cleanedResponse.substring(0, 200)}..."`);
  console.log(`PHASE 1 FULL RESPONSE LENGTH: ${cleanedResponse.length} characters`);
  
  return {
    provider,
    overallScore: finalScore,
    analysis: cleanedResponse,
    evaluationType: 'intelligence',
    formattedReport: cleanedResponse,
    rawResponse: phase1Response // DEBUG: Include raw response
  };
}

// COMPREHENSIVE PROTOCOL - Ultra-detailed evidence-based 4-phase analysis
export async function executeComprehensiveProtocol(
  text: string,
  provider: 'openai' | 'anthropic' | 'deepseek' | 'perplexity'
): Promise<any> {
  console.log(`COMPREHENSIVE 4-PHASE INTELLIGENCE EVALUATION: Analyzing ${text.length} characters`);
  console.log(`COMPREHENSIVE MODE - DETAILED EVIDENCE-BASED ANALYSIS WITH ${provider.toUpperCase()}`);
  
  const questions = EXACT_COMPLETE_QUESTIONS;
  
  // PHASE 1: COMPREHENSIVE DETAILED ANALYSIS (NO CHUNKING - analyze full text with extensive detail)
  console.log(`PHASE 1 COMPREHENSIVE: Starting ultra-detailed analysis of full text`);
  const phase1Prompt = createPhase1ComprehensivePrompt(text, questions);
  const phase1Response = await callLLMProvider(provider, [
    { role: 'user', content: phase1Prompt }
  ]);
  let phase1Score = extractScore(phase1Response);
  
  console.log(`PHASE 1 COMPREHENSIVE COMPLETE: Score ${phase1Score}/100, Response length: ${phase1Response.length} chars`);
  
  let phase2Response = '';
  let phase2Score = phase1Score;
  
  // PHASE 2: Comprehensive pushback if score < 95
  if (phase1Score < 95) {
    console.log(`PHASE 2 COMPREHENSIVE: Score ${phase1Score} < 95, applying detailed pushback`);
    const phase2Prompt = createPhase2ComprehensivePrompt(phase1Score, text, questions);
    phase2Response = await callLLMProvider(provider, [
      { role: 'user', content: phase2Prompt }
    ]);
    phase2Score = extractScore(phase2Response);
    console.log(`PHASE 2 COMPREHENSIVE COMPLETE: Revised score ${phase2Score}/100`);
  } else {
    console.log(`PHASE 2: Score ${phase1Score} >= 95, no pushback needed`);
    phase2Response = 'No pushback needed - score was already >= 95/100';
  }
  
  // PHASE 3: Walmart metric check
  console.log("PHASE 3: Walmart metric consistency check");
  const phase3Prompt = createPhase3Prompt(phase2Score);
  const phase3Response = await callLLMProvider(provider, [
    { role: 'user', content: phase3Prompt }
  ]);
  let phase3Score = extractScore(phase3Response);
  
  // PHASE 4: Final validation and acceptance
  console.log("PHASE 4: Final validation");
  const phase4Prompt = createPhase4Prompt();
  const phase4Response = await callLLMProvider(provider, [
    { role: 'user', content: phase4Prompt }
  ]);
  let finalScore = extractScore(phase4Response);
  
  // Use Phase 4 score, or best previous score if Phase 4 fails
  if (finalScore <= 0 || finalScore > 100) {
    // Only use fallback if Phase 4 completely failed to extract a score
    finalScore = Math.max(phase1Score, phase2Score, phase3Score);
    console.log(`PHASE 4 RESULT: Score extraction failed, using best previous score ${finalScore}/100`);
  } else {
    console.log(`PHASE 4 RESULT: Final score ${finalScore}/100`);
  }
  
  // Clean up response formatting
  const cleanResponse = (text: string) => {
    return text
      .replace(/\*{1,3}/g, '') // Remove asterisks
      .replace(/#{1,6}\s*/g, '') // Remove hashtags
      .replace(/\-{3,}/g, '') // Remove horizontal lines
      .replace(/\_{3,}/g, '') // Remove underscores
      .replace(/\n{3,}/g, '\n\n') // Reduce multiple newlines
      .trim();
  };

  // Detailed phase breakdown for comprehensive reports
  const phases = {
    phase1: {
      score: phase1Score,
      response: cleanResponse(phase1Response),
      prompt: "Comprehensive Evidence-Based Intelligence Evaluation"
    },
    phase2: {
      score: phase2Score,
      response: cleanResponse(phase2Response),
      applied: phase1Score < 95
    },
    phase3: {
      score: phase3Score,
      response: cleanResponse(phase3Response)
    },
    phase4: {
      score: finalScore,
      response: cleanResponse(phase4Response)
    }
  };

  return {
    provider,
    overallScore: finalScore,
    analysis: cleanResponse(phase1Response), // Comprehensive Phase 1 analysis
    phases, // Detailed breakdown of all phases
    evaluationType: 'intelligence',
    formattedReport: cleanResponse(phase1Response) // Comprehensive analysis
  };
}

// Unified function for backward compatibility 
export async function executeFourPhaseProtocol(
  text: string,
  provider: 'openai' | 'anthropic' | 'perplexity' | 'deepseek',
  evaluationType: string = 'intelligence',
  mode: 'normal' | 'comprehensive' = 'comprehensive'
): Promise<any> {
  if (mode === 'normal') {
    return executeNormalProtocol(text, provider);
  } else {
    return executeComprehensiveProtocol(text, provider);
  }
}